# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/122fXdJnytL35gEN70AUjBNstj0RoVOkn
"""

import streamlit as st
import re
import pandas as pd
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.sentiment import SentimentIntensityAnalyzer
from wordcloud import WordCloud
import emoji
import demoji
from transformers import pipeline
from googleapiclient.discovery import build
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')

# Set primary color
primary_color = '#CDCDCD'  # Replace with your desired color in hexadecimal format

# Set background color
background_color = '#CDCDCD'  # Replace with your desired color in hexadecimal format

# Set the overall theme
st.set_page_config(
    page_title="Youtube NLP App",
    page_icon="ðŸ“Š",
    layout="centered",
    initial_sidebar_state="auto",
)

# Apply the custom theme
css = f"""
    body {{
        color: {primary_color};
        background-color: {background_color};
    }}

    .css-1l02zno {{
        color: {primary_color};
    }}
"""
st.markdown(f'<style>{css}</style>', unsafe_allow_html=True)


stopw = stopwords.words('english')

# Function to extract the video ID
def extract_video_id(url):
    # Extract the video ID using the pattern
    match = re.search(r"watch\?v=([a-zA-Z0-9_-]{11})", url)
    if match:
        video_id = match.group(1)
        return video_id
    else:
        return None

# Function to categorize the sentiment
def categorize_sentiment(text):
    # Create an instance of the SentimentIntensityAnalyzer
    sid = SentimentIntensityAnalyzer()

    # Analyze sentiment
    sentiment_scores = sid.polarity_scores(text)

    # Categorize sentiment
    if sentiment_scores['pos'] > sentiment_scores['neg']:
        return 'Positive'
    else:
        return 'Negative'

# Function to preprocess comments and generate bag of words
def bag_of_words(data):
    bag_words = [a.lower() for i in data for a in re.split(' ', str(i)) if a.lower() not in stopw and len(a) > 3]
    return bag_words

# Define the Streamlit app
st.title("YouTube Comments Analysis ðŸ“Š")
st.write("Welcome to the *YouTube Comments Analysis App!* (to get the methodology explore the final section).")

# YouTube URL input
url = st.text_input("Enter YouTube video URL (e.g., https://www.youtube.com/watch?v=JvEas_zZ4fM)")

if st.button("Analyze"):
    # Extract the video ID
    video_id = extract_video_id(url)
    if video_id is None:
        st.error("Invalid YouTube URL. Please enter a valid URL.")
        st.stop()

    # API
    api_key = "AIzaSyBUajBi65Ou3NlU2hDr-XwhnHj8G4AwjJk"  # Replace with your actual API key
    youtube = build("youtube", "v3", developerKey=api_key)
    comments = []

    request = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=100,  # Maximum number of comments to retrieve per page
        textFormat="plainText"
    )

    while request:
        response = request.execute()

        for item in response["items"]:
            comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
            comments.append(comment)

        request = youtube.commentThreads().list_next(request, response)

    # Display comments
    st.subheader("Comments (first 10)")
    st.write(comments[:10])

    # Emojis information
    emojis = [emoji.emoji_list(i)[0]['emoji'] for i in comments if len(emoji.emoji_list(i)) > 0]
    emoji_name = [emoji.demojize(i) for i in emojis]
    df_emojis = pd.DataFrame({'emojis': emojis, 'emoji_name': emoji_name})
    df_emojis = pd.DataFrame(df_emojis.groupby(['emojis']).count()).reset_index()
    df_emojis.columns = ['emojis', 'values']
    df_emojis = df_emojis.sort_values('values', ascending=False).reset_index()[['emojis', 'values']]

    # Display emojis information
    st.subheader("Emojis Information")
    st.write(df_emojis.head())

    # Bag of words
    bag_words = bag_of_words(comments)
    fdist = nltk.FreqDist(bag_words)
    keys = fdist.keys()
    values = fdist.values()

    df_words = pd.DataFrame({'keys': keys, 'values': values}).sort_values(['values'], ascending=False).reset_index()[
        ['keys', 'values']]

    # Display word frequency
    st.subheader("Word Frequency")
    st.write(df_words.head(5))

    # Plot word frequency
    fig, ax = plt.subplots(figsize=(6, 4))
    plt.style.use('seaborn-white')
    plt.barh(df_words['keys'][:15], df_words['values'][:15], color='SlateBlue')
    plt.title('Word Frequency')
    ax.invert_yaxis()
    st.pyplot(fig)

    # Wordcloud
    bag_of_words_str = " ".join(bag_words)
    wc = WordCloud(
        background_color='white',
        stopwords=stopwords.words('english'),
        width=3000,
        height=2000,
        collocations=False,
        colormap='Dark2',
        max_words=40
    )
    wc.generate(bag_of_words_str)

    # Display wordcloud
    st.subheader("Wordcloud")
    plt.figure(figsize=[7, 7])
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    st.pyplot(plt)

    # Sentiment analysis
    df = pd.DataFrame(columns=['Sentence', 'Sentiment'])
    for comment in comments:
        sentiment = categorize_sentiment(comment)
        df = df.append({'Sentence': comment, 'Sentiment': sentiment}, ignore_index=True)

    # Sentiment distribution
    positive_count = df['Sentiment'].value_counts()['Positive']
    negative_count = df['Sentiment'].value_counts()['Negative']
    labels = ['Positive', 'Negative']
    sizes = [positive_count, negative_count]
    colors = ['green', 'red']

    # Display sentiment distribution
    st.subheader("Sentiment Distribution")
    fig, ax = plt.subplots()
    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    plt.axis('equal')
    plt.title('Sentiment Distribution')
    st.pyplot(fig)

st.write("Bonus: To get easly a url look at this table.")

def display_company_table():
    # Define the company data
    company_data = {
        'Video Name': ['Nassim Nicholas Taleb: How to Live in a World we Dont Understand','Chomsky-Foucault Debate on Power vs Justice (1971)',
        '15 Minutes of Kevin Hart Dad Jokes | Netflix Is A Joke','ChatGPT Tutorial: How to Use Chat GPT'],
        'URL': ['https://www.youtube.com/watch?v=iEnmjMgP_Jo&t=63s','https://www.youtube.com/watch?v=xpVQ3l5P0A4',
        'https://www.youtube.com/watch?v=gbxSpLDQehg','https://www.youtube.com/watch?v=Gaf_jCnA6mc']
    }

    # Create a DataFrame from the company data
    df = pd.DataFrame(company_data)

    # Display the DataFrame as a table in Streamlit
    st.table(df)

# Run the function to display the company table
display_company_table()

st.markdown("---")
st.write("The methodology summarized is:")
st.write("- The app was developed using python, streamlit and NLP techniques.")
st.write("- The app leverages the YouTube Data API to fetch comments related to the provided video ID.")
st.write("- With NLTK and emoji library the word-emoji frequency is getted.")
st.write("- The Sentiment Analysis is done with VADER model.")
st.markdown("---")
st.markdown("Created by Oscar Gomez - oscar.gomezr0414@gmail.com")
